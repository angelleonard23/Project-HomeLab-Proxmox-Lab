# ğŸ“‚ Enterprise Home Lab & Security Infrastructure

## ğŸ¯ Projekt-Vision
Systematische Transformation einer Consumer-Netzwerkumgebung in eine hochverfÃ¼gbare, segmentierte Enterprise-Lab-Struktur. Dieses Projekt dient als Proof-of-Concept fÃ¼r moderne IT-Infrastrukturen, mit Fokus auf Virtualisierung, Advanced Firewalling (pfSense) und die Migration hin zu einer Container-basierten Microservice-Architektur.

---

## ğŸ’» Hardware-Stack (Physical Layer)
Die Basis bildet ein performanter Mini-Server, der speziell auf Effizienz und Multi-Core-Workloads ausgelegt ist:

* **Hypervisor-Host:** AOOSTAR WTR PRO
    * **CPU:** AMD Ryzen 7 5825U (8 Kerne / 16 Threads)
    * **RAM:** 64GB DDR4 (fÃ¼r hohen VM-Konsolidierungsgrad)
* **Networking:**
    * **Core-Router:** TP-Link Archer AX18 (Gateway zum ISP)
    * **Uplink:** Magenta Fiber Box (Anbindung via Double-NAT / DMZ-Vorhaltung)
    * **Infrastruktur:** TP-Link RE330 OneMesh zur stabilen Anbindung des Lab-Standorts.

![Nachweis: Proxmox Host-Ressourcen & Auslastung](./img/Proxmox_Dashboard_CPU_RAM_Ãœbersicht.png)

---

## ğŸŒ Netzwerk-Topologie & Virtualisierungs-Design
Die logische Trennung erfolgt auf Hypervisor-Ebene durch den Einsatz dedizierter virtueller Bridges, um eine strikte WAN/LAN-Trennung zu erzwingen.

* **Virtualisierungsschicht:** Proxmox VE (Debian-basiert)
* **Netzwerk-Abstraktion:**
    * `vmbr0` (WAN): Physischer Uplink zur AuÃŸenwelt.
    * `vmbr1` (Isolated LAN): Exklusives Backend-Netz fÃ¼r die Lab-Infrastruktur.

### Logischer Datenfluss
```mermaid
graph TD
    subgraph "Public Internet"
        ISP[Magenta Fiber Box]
    end

    subgraph "Physical Perimeter"
        Archer[TP-Link Archer AX18 - 192.168.1.1]
        Aoostar[AOOSTAR WTR PRO - Proxmox Host]
    end

    subgraph "Virtual Secure Environment (Proxmox)"
        WAN_Bridge((vmbr0 - WAN))
        LAN_Bridge((vmbr1 - Isoliertes LAN))
        
        pfSense[pfSense Firewall]
        Mint[Linux Mint Management VM]
    end

    ISP --- Archer
    Archer --- WAN_Bridge
    WAN_Bridge --- pfSense
    pfSense --- LAN_Bridge
    LAN_Bridge --- Mint

`````
# ğŸ“‚ Phase 1: pfSense-Core, VLAN-Design & Security-Baseline

## ğŸ¯ Zielsetzung
Aufbau einer hochverfÃ¼gbaren Virtualisierungsplattform (Proxmox) und Implementierung einer zentralen Firewall-Instanz (pfSense). Fokus liegt auf der Etablierung einer sicheren Netzwerk-Topologie durch VLAN-Segmentierung, der Absicherung administrativer ZugÃ¤nge und dem proaktiven Schutz durch DNS-Filterung.

---

## ğŸ—ï¸ 1. Infrastruktur-Architektur (Hypervisor)
Das gesamte Labor wird auf einem Proxmox VE-Node (AMD Ryzen 7) betrieben. Die Architektur wurde auf maximale Ressourcen-Effizienz und logische Trennung optimiert.

* **Firewall:** pfSense CE als zentrales Security-Gateway (Routing/NAT/Filterung).
* **Management-Node:** Linux Mint Xfce Edition zur ressourcenschonenden Administration.
* **Service-Node:** Debian 13 "Trixie" als Headless-System fÃ¼r den Web-Service.

| Komponente | Interface | IP-Adresse | Subnetzmaske | Zweck |
| :--- | :--- | :--- | :--- | :--- |
| **pfSense WAN** | `vmbr0` | `192.168.1.136` | `/24` | Uplink zum physischen Gateway |
| **pfSense LAN** | `vmbr1` | `10.0.0.1` | `/24` | Management-Gateway |
| **Web-VLAN 20** | `VLAN 20` | `10.0.20.1` | `/24` | Isoliertes Server-Segment |

---

## ğŸš¦ 2. Netzwerk-Segmentierung & VLAN-Design
Zur Reduzierung der AngriffsflÃ¤che wurde der Webserver vom Management-Netz isoliert (VLAN 20).

* **Interface-Isolation:** Implementierung des WEBSERVER-Interfaces auf VLAN-ID 20.
* **Firewall-Logic (Inter-VLAN-Routing):**
    * **Directional Control:** Das Management-VLAN darf auf den Webserver zugreifen (Pull-Prinzip).
    * **Isolation:** Eine restriktive Block-Regel unterbindet jegliche Kommunikation vom Webserver-Segment in das LAN-Segment (Push-PrÃ¤vention).

![Nachweis: Firewall-Ruleset zur Isolation des Server-Segments](./img/Firewall_Rules_WEBSERVER_subnets_Destination_LAN_subnets.png)

---

## ğŸ”’ 3. Perimeter-Hardening & pfBlockerNG
Die pfSense wurde Ã¼ber die Standardkonfiguration hinaus gehÃ¤rtet, um eine proaktive Sicherheits-Baseline zu schaffen.

* **Management-Isolation:** Verschieben des Web-GUI-Ports auf **Port 8443**. Dies verhindert Konflikte mit Web-Diensten (80/443) und erschwert das Discovery der Admin-Schnittstelle.
* **DNS-Security (pfBlockerNG):** * Implementierung von DNSBL-Filtern zur automatisierten Blockierung von Telemetrie, Tracking und Malware-Domains.
    * **Validierung:** Nachweis der DNS-Umleitung (Sinkhole) bÃ¶sartiger Domains auf die interne VIP (10.10.10.1).

![Test: DNS-Blocking-Validierung via nslookup](./img/pfsense_pfblocker_test.jpg)

---

## ğŸŒ 4. Service-Publishing & NAT-Loopback
Die VerÃ¶ffentlichung des Web-Dienstes erfolgt via Destination NAT (DNAT) unter BerÃ¼cksichtigung der Sicherheits-Zonen.

* **Inbound-Access:** Portweiterleitung von WAN (80/443) auf das interne Debian-Target (`10.0.20.50`).
* **Routing-Analyse:** Trennung von internem und externem Traffic-Flow zur Vermeidung von Informationspreisgabe Ã¼ber administrative Schnittstellen nach auÃŸen.

![Nachweis: Operatives pfSense-Dashboard mit verifiziertem HTTPS-Zugriff](./img/pfsense_dashboard_live.jpg)

---

## ğŸ§ª 5. Validierung & Performance-Audit
Der finale Testlauf bestÃ¤tigt die IntegritÃ¤t der gewÃ¤hlten Architektur:

* **KonnektivitÃ¤t:** Erfolgreicher ICMP-Ping (8.8.8.8) verifiziert den Internet-Uplink.
* **Security-Audit:** 100% Paketverlust bei Cross-VLAN-Zugriffen (Web â®• LAN).
* **Effizienz:** Minimale Host-Last durch den Einsatz von Headless-Services und dem schlanken Xfce-Management-Client.

![Validierung: Nachweis der Netzisolierung (Paketverlust-Test)](./img/DMZ_Isolierungstest_Fail.png)

---

## âœ… 6. Projektabschluss Phase 1
Die Kern-Infrastruktur ist bereit. Die Netzwerk-Topologie ist nach dem Least-Privilege-Prinzip segmentiert und durch pfBlockerNG proaktiv geschÃ¼tzt. Alle initialen Credentials wurden in der `vault_passwords.yml` dokumentiert.

---

# ğŸ“‚ Phase 2: Webserver-Migration, DMZ-Isolation & Hardening

## ğŸ¯ Zielsetzung
Physische und logische Migration des Debian-Webservers in eine isolierte Demilitarized Zone (DMZ). Fokus liegt auf der Implementierung eines restriktiven Firewall-Regelwerks, der HÃ¤rtung des administrativen Zugriffs (SSH) sowie der Initialisierung eines abgesicherten Datenbank-Backends.

---

## ğŸ—ï¸ 1. Netzwerk-Migration & IP-Stack
Die Migration in die DMZ (VLAN 30) erforderte eine Neukonfiguration der Netzwerkschnittstellen zur GewÃ¤hrleistung der Segmentierung.

* **Schnittstellen-Konfiguration:** Umstellung auf statische Adressierung (`10.0.30.50`) via `/etc/network/interfaces`.
* **Routing-Validierung:** Konfiguration des DMZ-Gateways (`10.0.30.1`) und Verifizierung der Layer-2-KonnektivitÃ¤t mittels `ip a`.

![Nachweis: Statische IP-Zuweisung und Interface-Status](./img/schreenshot_etc_network_interfaces.png)

---

## ğŸš¦ 2. pfSense: NAT-Orchestrierung & Perimeter-Security
Die VerÃ¶ffentlichung des Web-Dienstes erfolgte nach dem Prinzip der minimalen Exposition.

* **Port-Forwarding (Destination NAT):** Granulare Umleitung von Inbound-Traffic (TCP 80/443) von der WAN-Schnittstelle auf das DMZ-Target.
* **Firewall-Regelsatz (Isolation):**
    * **Inter-VLAN-Blocking:** Explizites Verbot jeglicher Kommunikation aus der DMZ in das interne LAN-Segment.
    * **Management-Protection:** Unterbindung von Zugriffen auf sensible Infrastruktur-Schnittstellen.
    * **Egress-Control:** Zulassen von HTTP/HTTPS-Traffic fÃ¼r System-Updates (Ubuntu Repositories).

![Policy: pfSense NAT-Rules und restriktive DMZ-Filterung](./img/screenshot_port_forwarding.png)

---

## ğŸ”’ 3. System-Hardening & Brute-Force-PrÃ¤vention
Zur Absicherung des Hosts gegen externe Angriffsvektoren wurden mehrere Schutzebenen (Defense in Depth) implementiert.

* **SSH-Hardening:** * Deaktivierung des Root-Logins (`PermitRootLogin no`) zur Erschwerung von Privilege-Escalation-Versuchen.
    * Empfehlung zur Nutzung kryptografischer SchlÃ¼sselpaare (RSA/Ed25519) anstelle von PasswÃ¶rtern.
* **Intrusion Prevention:** Installation und Konfiguration von **Fail2Ban**. Der Dienst Ã¼berwacht Logfiles (`auth.log`) und sperrt IP-Adressen automatisiert bei verdÃ¤chtigen Login-Mustern.

![Nachweis: GehÃ¤rtete SSH-Konfiguration und Fail2Ban-Integration](./img/ss23_permitrootlogin_no.png)

---

## ğŸ’¾ 4. Datenbank-Backend & PHP-Initialisierung
Die Bereitstellung der MariaDB-Instanz erfolgte unter BerÃ¼cksichtigung von Best-Practice-Sicherheitsstandards.

* **DB-Hardening:** DurchfÃ¼hrung der `mariadb-secure-installation` zur Entfernung von Test-Datenbanken und anonymen Usern.
* **Identity Management:** Erstellung eines dedizierten Applikations-Benutzers (`webuser`) mit eingeschrÃ¤nkten Privilegien auf die `projekt_db`.
* **Connectivity-Validierung:** Implementierung eines PHP-Testskripts zur Verifizierung des Handshakes zwischen Webserver und Datenbank-Backend.

![Validierung: Erfolgreiche Datenbank-KonnektivitÃ¤t via PHP-Backend](./img/ss17_datenbank_webseite.png)

---

## ğŸ§ª 5. Compliance-Test & Sicherheits-Audit
Der Erfolg der Segmentierung wurde durch strukturierte KonnektivitÃ¤ts-Checks nachgewiesen:

1. **WAN-Reachability:** Erfolgreicher Web-Zugriff Ã¼ber die WAN-IP.
2. **Egress-Validierung:** Funktionaler Internet-Ping (8.8.8.8) fÃ¼r Updates.
3. **Isolations-Check:** Nachweis von **100% Packet Loss** bei Zugriffsversuchen auf das interne LAN (10.0.10.1).

![Audit: Verifizierte Blockierung des Cross-VLAN-Traffics](./img/ss13_connection_test.png)

---

## âœ… 6. Projektabschluss Phase 2
Die Web-Infrastruktur befindet sich nun in einem gehÃ¤rteten Betriebszustand innerhalb einer isolierten Sicherheitszone. Alle sicherheitsrelevanten Zugangsdaten sind in der `vault_passwords.yml` zentralisiert.

---

# ğŸ“‚ Phase 3: Deployment des Application-Stacks (LAMP)

## ğŸ¯ Zielsetzung
Transformation des statischen Webservers in eine dynamische Applikationsumgebung. Fokus liegt auf dem Aufbau eines resilienten LAMP-Stacks (Linux, Apache, MariaDB, PHP), der Implementierung sicherer Datenbank-Schnittstellen und der Absicherung der Kommunikation via TLS.

---

## ğŸ—ï¸ 1. Architektur-Review (LAMP-Komponenten)
Die Infrastruktur wurde gezielt fÃ¼r den Betrieb in der DMZ (VLAN 30) optimiert und gehÃ¤rtet.

* **OS-Ebene:** Ubuntu Server als stabile Host-Basis (`10.0.30.50`).
* **Web-Frontend:** Apache HTTP Server mit aktivierten Modulen fÃ¼r PHP-Prozessierung und SSL.
* **Backend-Logik:** PHP 8.x zur Verarbeitung der CRUD-Operationen und Session-Handling.
* **Data-Layer:** MariaDB als relationaler Datenspeicher fÃ¼r die Logbuch-Metadaten.

![Nachweis: Operative Web-OberflÃ¤che mit aktiven DatensÃ¤tzen](./img/webseite_EintrÃ¤ge.png)

---

## ğŸ’¾ 2. Datenbank-Design & Security-Hardening
Bei der Konfiguration der Datenbank `projekt_db` wurde besonderer Wert auf Datensparsamkeit und Schutz gegen Web-Vulnerabilities gelegt.

* **Schema-Design:** Definition der Tabelle `logbuch` mit optimierten Datentypen fÃ¼r Zeitstempel und Dateipfade.
* **Applikations-Sicherheit:** Konsequente Nutzung von **Prepared Statements** in der PHP-Logik zur effektiven Unterbindung von SQL-Injection-Angriffen.
* **Access Control:** Verwendung dedizierter Datenbank-User anstelle des Root-Accounts fÃ¼r den Applikationszugriff.

![Schema-Audit: Tabellenstruktur der MariaDB-Instanz](./img/screenshot_mariadb2.png)

---

## âš™ï¸ 3. Service-Konfiguration & Troubleshooting
Die Bereitstellung erforderte tiefe Eingriffe in die Webserver-Konfiguration zur GewÃ¤hrleistung der Applikations-FunktionalitÃ¤t.

* **Directory Security:** Anpassung der `000-default.conf` (`AllowOverride All`), um die Nutzung von `.htaccess`-Files fÃ¼r URL-Rewriting und Zugriffsschutz zu ermÃ¶glichen.
* **TransportverschlÃ¼sselung:** Implementierung von TLS zur Absicherung des administrativen Logins und der DatenÃ¼bermittlung.
* **Fehleranalyse:** Behebung von initialen Berechtigungsproblemen im Upload-Verzeichnis (`/var/www/html/uploads`) durch granulare Vergabe von Schreibrechten fÃ¼r den `www-data` User.

![Nachweis: Erfolgreicher HTTPS-Handshake und Apache-Validierung](./img/meme_leonardo.png)

---

## ğŸ§ª 4. Funktionale Validierung (CRUD-Audit)
Der volle Funktionszyklus der Applikation wurde erfolgreich gegen die Anforderungen geprÃ¼ft:

1. **Create/Read:** Upload-Tests von Bilddaten inkl. automatischer Zeitstempel-Generierung.
2. **Delete:** Verifizierung der Datenkonsistenz beim Entfernen von EintrÃ¤gen Ã¼ber die ID.
3. **Session-Management:** Validierung des passwortgeschÃ¼tzten Bereichs zur Absicherung der administrativen Funktionen.

![Code-Audit: Implementierung der PHP-Backend-Logik](./img/schreenshot_code_log_php.png)

---

## âœ… 5. Projektabschluss Phase 3
Der Application-Stack ist vollstÃ¤ndig einsatzbereit und bildet die funktionale Basis des Projekts. Alle administrativen PasswÃ¶rter und Datenbank-Credentials wurden sicher in der Datei `vault_passwords.yml` hinterlegt.

---

# ğŸ“‚ Phase 4: Disaster Recovery & Automatisierung

## ğŸ¯ Zielsetzung
GewÃ¤hrleistung der Business Continuity durch die Implementierung eines automatisierten Backup-Frameworks. Fokus liegt auf der Sicherstellung der DatenintegritÃ¤t, der Einhaltung von Aufbewahrungsfristen (Retention Policy) und der Validierung von Wiederherstellungsszenarien.

---

## ğŸ›¡ï¸ 1. Infrastruktur-HÃ¤rtung (Backup-Repository)
Zum Schutz sensibler Datenbank-Dumps wurde ein dediziertes Backup-Verzeichnis auÃŸerhalb des Web-Wurzelverzeichnisses (`DocumentRoot`) etabliert.

* **Sicherheitskonzept:** Implementierung restriktiver Berechtigungen (`chmod 700`), um den Zugriff exklusiv auf den administrativen User zu beschrÃ¤nken.
* **PrÃ¤vention:** Ausschluss von Information Disclosure durch physische Trennung von Backup- und Web-Inhalten.

![Nachweis: Verzeichnisstruktur und restriktive Berechtigungsebene](./img/Screenshot_Verzeichnis.png)

---

## ğŸ“œ 2. Entwicklung der Backup-Logik (Bash-Scripting)
Die Sicherung erfolgt Ã¼ber ein modular aufgebautes Bash-Skript (`backup_logbuch.sh`), welches sowohl strukturierte (SQL) als auch unstrukturierte Daten (Images) konsolidiert.

* **Kern-Features:**
    * **Datenbank-Export:** Konsistenter Export via `mysqldump` zur Sicherung der SQL-IntegritÃ¤t.
    * **Komprimierung:** Effiziente Archivierung der Media-Assets mittels `tar -czf` zur Reduzierung des Speicherbedarfs.
    * **Automatisierte Rotation:** Implementierung einer 7-tÃ¤gigen Vorhaltezeit (`find -mtime +7`), um unkontrolliertes Datenwachstum im Repository zu verhindern.

![Code-Review: VollstÃ¤ndige Implementierung der Backup-Logik](./img/Screenshot_Backup_Code.png)

---

## âš™ï¸ 3. Automatisierung (Cron-Orchestrierung)
Zur Minimierung menschlicher Fehlerquellen wurde der Prozess in den System-Scheduler `cron` integriert.

* **Scheduling:** AusfÃ¼hrung tÃ¤glich um **03:00 Uhr**, um die Systemlast wÃ¤hrend der Hauptbetriebszeit zu minimieren.
* **Persistence:** Der Eintrag in der Crontab garantiert eine lÃ¼ckenlose Historie der Sicherungspunkte.

![Konfiguration: System-Scheduler (Crontab) mit Backup-Intervall](./img/Screenshot_crontab_l.png)

---

## ğŸ§ª 4. Validierung & Disaster Recovery Test
Ein Backup ist wertlos ohne validierte Wiederherstellung. Die FunktionsfÃ¤higkeit wurde durch einen simulierten Daten-Restore erfolgreich nachgewiesen.

* **IntegritÃ¤ts-Check:** Verifizierung der DateigrÃ¶ÃŸen und Zeitstempel nach dem Skript-Durchlauf.
* **Recovery-Validierung:** Erfolgreicher Re-Import eines SQL-Dumps in die MariaDB-Instanz zur Wiederherstellung des produktiven Applikationsstatus.

![Nachweis: Manueller Testlauf und Datei-Validierung](./img/Screenshot_backup_validierung.png)

---

## âœ… 5. Projektabschluss Phase 4
Mit der Automatisierung der Backups ist die Applikation gegen Datenverlust abgesichert. Die Trennung von Sicherungsdaten und Produktivsystem folgt den Best Practices der IT-Sicherheit. Alle relevanten Datenbank-Credentials fÃ¼r das Skript wurden in der `vault_passwords.yml` hinterlegt.

---

# ğŸ“‚ Phase 5: Containerisierung & Microservice-Migration

## ğŸ¯ Zielsetzung
Transformation der monolithischen LAMP-Installation in eine containerisierte Microservice-Architektur. Fokus liegt auf der Isolation der Dienste via Docker, der Sicherstellung von Datenpersistenz und der Erstellung optimierter Custom-Images.

---

## ğŸ—ï¸ 1. Orchestrierung (Infrastructure as Code)
Die Definition des Application-Stacks erfolgte deklarativ mittels **Docker Compose**, um eine reproduzierbare Umgebung zu schaffen.

* **Service-Isolation:** Trennung von Applikations-Logik (PHP-Apache) und Datenbank-Layer (MariaDB).
* **Netzwerk-Abstraktion:** Kommunikation Ã¼ber ein isoliertes Bridge-Netzwerk; Datenbank-Port bleibt ohne externes Mapping (Internal-only).
* **Port-Strategie:** Mapping von Host-Port 8080 auf Container-Port 80 zur Vermeidung von Privileg-Konflikten auf dem Host.

![Konfiguration: Docker-Compose-Stack mit Volume-Mapping](./img/docker-compose-yml.png)

---

## ğŸ› ï¸ 2. Custom Image Engineering (Dockerfile)
Da das offizielle PHP-Basisimage keine nativen MySQL-Treiber enthÃ¤lt, wurde ein maÃŸgeschneidertes Image entwickelt.

* **Build-Prozess:** Automatisierte Installation der `mysqli`-Erweiterung via Docker-PHP-Scripts.
* **Troubleshooting:** Identifikation und Behebung von Syntaxfehlern im Build-Layer.
* **Optimierung:** Minimierung der Image-GrÃ¶ÃŸe durch gezielte Installation notwendiger AbhÃ¤ngigkeiten.

![Nachweis: Erfolgreicher Multi-Layer-Build des Custom PHP-Images](./img/Costum_image_build.png)

---

## ğŸ’¾ 3. Daten-Migration & Volume-Persistenz
Die grÃ¶ÃŸte Herausforderung bestand im Transfer der bestehenden DatensÃ¤tze in die persistente Container-Struktur.

* **Incident-Handling:** LÃ¶sung von Konflikten bei der Initialisierung der Umgebungsvariablen (`MYSQL_ROOT_PASSWORD`).
* **Migration-Workflow:** 1. Bereinigung der Alt-Volumes zur Korrektur von Initialisierungskonflikten.
    2. Hot-Import des SQL-Dumps direkt in den laufenden Datenbank-Container via STDIN.
* **Persistenz-Konzept:** Nutzung von Bind-Mounts zur Entkopplung der Datenbank-Files (`./db_data`) vom Container-Lifecycle.

![Nachweis: SQL-Import und Datenbank-Initialisierung](./img/Docker_Datenbank_Migration.png)

---

## ğŸ§ª 4. Validierung (Integrations-Test)
Der Erfolg der Migration wurde durch einen End-to-End-Funktionstest verifiziert.

* **Connectivity-Check:** Validierung des Datenbank-Handshakes via PHP unter Verwendung des internen Docker-DNS (Host: `db`).
* **Frontend-Audit:** BestÃ¤tigung der DatenintegritÃ¤t durch Abfrage der migrierten DatensÃ¤tze im Browser.

![Validierung: BestÃ¤tigter Datenbank-Connect im Web-Frontend](./img/MariaDB_Container_Browser.png)

---

## âœ… 5. Projektabschluss Phase 5
Die Anwendung ist nun vollstÃ¤ndig portabel und skaliert unabhÃ¤ngig vom Host-Betriebssystem. Die gewonnenen Erkenntnisse Ã¼ber **Docker-Netzwerke** und **Volume-Initialisierung** bilden das Fundament fÃ¼r zukÃ¼nftige Skalierungsszenarien. SÃ¤mtliche PasswÃ¶rter wurden konsistent in der `vault_passwords.yml` dokumentiert.

---

# ğŸ“‚ Phase 6: Client-Provisionierung & DomÃ¤nenintegration

## ğŸ¯ Zielsetzung
Deployment einer Windows 11 Pro Instanz (CL-01-WIN11) als verwalteter Endpunkt innerhalb der DomÃ¤ne `projekt.local`. Fokus liegt auf der Optimierung fÃ¼r die virtualisierte Umgebung (Proxmox), der Etablierung einer statischen Netzwerkkonfiguration und der Validierung des zentralen Policy-Managements via GPO.

---

## ğŸ—ï¸ 1. Virtualisierungs-Design (Proxmox-Spezifikationen)
Um maximale Systemperformance und StabilitÃ¤t unter Windows 11 zu gewÃ¤hrleisten, wurde das Hardware-Profil gezielt auf die Virtualisierungsumgebung abgestimmt:

* **Sicherheits-Features:** Implementierung eines virtuellen **TPM 2.0 Chips** und **OVMF (UEFI)** zur ErfÃ¼llung der Windows-IntegritÃ¤tsanforderungen.
* **Storage-Optimierung:** Einsatz des **VirtIO SCSI Single** Controllers mit aktiviertem *Discard*-Support zur effizienten SSD-Nutzung.
* **Netzwerk-Segmentierung:** Zuweisung des **VLAN-Tags 30** (Client-VLAN) auf der virtuellen Bridge zur strikten Layer-2-Isolierung.

---

## ğŸ› ï¸ 2. OS-Deployment & Treiber-Integration
Der Installationsprozess wurde manuell optimiert, um die AbhÃ¤ngigkeit von Standard-Treibern zu umgehen und ein gehÃ¤rtetes lokales Setup zu erzielen:

* **Injected Drivers:** Einbindung der `virtio-win`-Treiber wÃ¤hrend des Setups (Pfad: `vioscsi\w11\amd64`), um die Performance des Storage-Stacks zu maximieren.
* **Deployment-Hardening:** Umgehung des Online-Account-Zwangs via `OOBE\BYPASSNRO`, um die Kontrolle Ã¼ber lokale Benutzerkonten zu behalten.
* **Guest-Tools:** Installation der QEMU Guest Agents zur prÃ¤zisen Steuerung und Ressourcenauswertung durch den Hypervisor.

---

## ğŸ”— 3. Netzwerk-Audit & DomÃ¤nenbeitritt
Die Integration in die Active Directory-Struktur erfolgte Ã¼ber eine dedizierte Konfigurations- und Testsequenz:

* **DNS-IntegritÃ¤t:** Statische Zuweisung des DC-01 (`10.0.30.100`) als autoritativen DNS-Server zur GewÃ¤hrleistung der SRV-Record-AuflÃ¶sung.
* **Domain Join:** Beitritt zur DomÃ¤ne `projekt.local` unter Verwendung dedizierter administrativer Credentials.

![Nachweis: Erfolgreiche DomÃ¤nenintegration des Clients](./img/Systemeigenschaften.png)

---

## âš™ï¸ 4. Active Directory Governance & GPO-Validierung
Nach der Aufnahme des Clients wurde die organisatorische Struktur innerhalb des AD verfeinert und die zentrale Richtlinienkompetenz geprÃ¼ft:

* **OU-Struktur:** Verschieben des Computer-Objekts in die Ziel-OU `Angel_Projekt -> Computer` zur Anwendung spezifischer Richtlinien.
* **GPO-Enforcement:** Implementierung der `GPO_Sicherheit_Login` (Interaktive Anmeldung).
* **BeweisfÃ¼hrung:** Nachweis der Richtlinien-Ãœbernahme (`gpupdate /force`) durch das Erscheinen eines administrativen Info-Banners beim Systemstart.

![Nachweis: Korrekte Objekt-Platzierung in der AD-Struktur](./img/CL-01-WIN11_AD.png)
![Validierung: Erzwungene GPO-Login-Nachricht am Client](./img/Willkommen_Hinweis.png)

---

## âœ… 5. Projektabschluss Phase 6
Die Client-Infrastruktur ist nun vollstÃ¤ndig zentral verwaltet. Die erfolgreiche Kommunikation zwischen VLAN 30 (Client) und dem Server-Segment wurde durch DNS- und GPO-Tests bestÃ¤tigt. Das System ist bereit fÃ¼r das Deployment netzwerkbasierter Dienste.

---

# ğŸ“‚ Phase 7: Zentrales Fileservice-Management & Datensicherheit

## ğŸ¯ Zielsetzung
Implementierung einer zentralen Dateiablage auf dem Domain Controller (DC-01) zur strukturierten Bereitstellung von Projektdaten. Fokus liegt auf der automatisierten Bereitstellung via Group Policy (GPO), der Berechtigungssteuerung nach Industriestandard und der Absicherung gegen Datenverlust durch Point-in-Time-Recovery.

---

## ğŸ” 1. Berechtigungsmanagement (AGDLP-Prinzip)
Die Zugriffssteuerung wurde strikt nach dem AGDLP-Modell (Account, Global Group, Domain Local Group, Permission) umgesetzt, um Skalierbarkeit und Revisionssicherheit zu gewÃ¤hrleisten.

* **Ressourcen-Struktur:** Zentraler Share unter `C:\Shares\Projektdaten`.
* **IdentitÃ¤tsmanagement:** Erstellung der globalen Sicherheitsgruppe `G_Projekt_Vollzugriff`.
* **Berechtigungs-Level:** * **NTFS:** Vergabe der Rechte "Ã„ndern", "Lesen" und "Schreiben" an die dedizierte Gruppe.
    * **Share-Level:** "Full Control" fÃ¼r authentifizierte Benutzer, wobei die effektive ZugriffsbeschrÃ¤nkung Ã¼ber die restriktivere NTFS-Ebene erfolgt.

![Nachweis: NTFS-Berechtigungsstruktur und Gruppenbindung](./img/NTFS-Berechtigungen.png)

---

## âš™ï¸ 2. Automatisierung (Group Policy Object)
Zur Steigerung der User-Experience und Standardisierung der ArbeitsplÃ¤tze wurde die Gruppenrichtlinie `GPO_DriveMapping_P` implementiert.

* **Mechanismus:** GPO-Preferences unter `User Configuration > Preferences > Windows Settings > Drive Maps`.
* **Parameter:** * **Aktion:** Update (sicherstellt die Konsistenz bei jedem Login).
    * **UNC-Pfad:** `\\DC-01\Projektdaten`.
    * **Mount-Point:** Fest definierter Laufwerkbuchstabe **P:**.

![Nachweis: GPO-Konfiguration fÃ¼r die automatisierte Netzlaufwerk-Zuweisung](./img/GPO_Laufwerkszuordnung.png)

---

## ğŸ›¡ï¸ 3. Data Protection (Volume Shadow Copies)
Als proaktive MaÃŸnahme gegen versehentliches LÃ¶schen oder Korruption wurde der Volume Shadow Copy Service (VSS) konfiguriert.

* **Technologie:** Point-in-Time-Snapshots auf Blockebene.
* **Self-Service-Recovery:** ErmÃ¶glicht Benutzern die eigenstÃ¤ndige Wiederherstellung von "VorgÃ¤ngerversionen" ohne administrativen Support.
* **System-Resilienz:** Reduzierung der Recovery Time Objective (RTO) fÃ¼r granulare Dateiwiederherstellungen.

![Status: Aktivierte Schattenkopien auf dem Datei-Volume](./img/Schattenkopien.png)

---

## ğŸ§ª 4. Validierung & User Acceptance Test (UAT)
Die FunktionalitÃ¤t der Infrastruktur wurde am Windows 11 Client (CL-01) unter einem Standard-Benutzer verifiziert:

1. **Policy-Enforcement:** Nachweis der GPO-Ãœbernahme via `gpupdate /force`.
2. **Mount-Validierung:** Automatisches Erscheinen des Laufwerks P: im Explorer-Namespace.
3. **Integrations-Test:** Erfolgreiche DurchfÃ¼hrung von I/O-Operationen (Erstellen/Ã„ndern von Testfiles) innerhalb des Shares.

![Nachweis: Erfolgreiches GPO-Processing und Laufwerks-Mount am Client](./img/Client-Validierung.png)

---

## ğŸ§¹ 5. Dokumentation & Status
Die Fileserver-Rolle ist vollstÃ¤ndig in die DomÃ¤nenstruktur integriert. Alle Berechtigungen und GPO-Einstellungen wurden persistiert und fÃ¼r den operativen Betrieb freigegeben.

---

# ğŸ“‚ Phase 8: Fortgeschrittene Administration & Ressourcen-Governance

## ğŸ¯ Zielsetzung
Implementierung proaktiver Management-Strukturen auf dem Fileserver (DC-01). Fokus liegt auf der Sicherstellung der SystemstabilitÃ¤t durch Kontingentverwaltung (Quotas), Durchsetzung von Unternehmensrichtlinien via Dateiscreening sowie Etablierung eines lokalen Monitoring-Workflows.

---

## ğŸ’¾ 1. KapazitÃ¤tsmanagement (Storage Quotas)
Um Service-Unterbrechungen durch unkontrolliertes Datenwachstum zu verhindern, wurde ein striktes Kontingentmanagement eingefÃ¼hrt.

* **Technologie:** Ressourcenmanager fÃ¼r Dateiserver (FSRM).
* **Implementierung:** * **Zielpfad:** `C:\Shares\Projektdaten` (Netzlaufwerk P:).
    * **Quota-Typ:** "Hard Quota" (5 GB). Das Ãœberschreiten des Limits wird systemseitig unterbunden.
* **Governance:** Einsatz einer standardisierten Vorlage (`Limit_5GB_Projektdaten`) zur GewÃ¤hrleistung der Revisionssicherheit.

![Konfiguration: Definition des harten 5GB-Kontingents](./img/Kontingent-Konfiguration.png)

---

## ğŸš« 2. Compliance & Dateiscreening
Zur technischen Durchsetzung von Nutzungsrichtlinien wurde ein aktives Dateiscreening implementiert.

* **Funktion:** Inhaltsbasierte Filterung statt reiner Dateiendung-PrÃ¼fung.
* **Restriktion:** Blockieren von nicht-geschÃ¤ftskritischen Dateigruppen (Audio-, Video- und Bilddateien) auf dem Projektlaufwerk.
* **Ergebnis:** Effektive Reduzierung von Schatten-Backups privater Medien und Schutz des produktiven Speichers.

![Policy: Aktive DateiprÃ¼fungseigenschaften fÃ¼r Mediendateien](./img/DateiprÃ¼fungs-Eigenschaften.png)

---

## ğŸ“Š 3. Proaktives Monitoring & Incident-Response
Da in der isolierten Testumgebung kein SMTP-Relay zur VerfÃ¼gung steht, wurde ein lokaler Monitoring-Workflow etabliert.

* **Schwellenwert-Analyse:** Automatische Trigger-AuslÃ¶sung bei **85 %** Speicherauslastung.
* **Event-Logging:** Umleitung der Warnmeldungen in das Windows-Ereignisprotokoll (Quelle: `SRMSVC`).
* **Sichtbarkeit:** Administratoren kÃ¶nnen Ã¼ber die Ereignisanzeige EngpÃ¤sse identifizieren, bevor diese den Betrieb beeintrÃ¤chtigen.

![Monitoring: Konfiguration der Schwellenwerte und Warnmeldungen](./img/Schwellenwert_SMTP-Hinweis.png)

---

## ğŸ§ª 4. Validierung & Wirksamkeitsnachweis
Der Nachweis der technischen Durchsetzung erfolgte durch gezielte Funktionstests:

1. **Compliance-Test (Client):** Ein Kopierversuch unzulÃ¤ssiger Dateitypen auf das Laufwerk P: resultiert in einem "Access Denied". Die NTFS-Rechte werden hierbei durch die FSRM-Policy Ã¼berschrieben.
2. **Audit-Trail (Server):** Verifizierung der Generierung von Warnereignissen in der Ereignisanzeige nach Erreichen der Schwellenwerte.

![Audit: Nachweis der Richtliniendurchsetzung in der Ereignisanzeige](./img/Ereignisanzeige_Monitoring.png)

---

## ğŸ§¹ 5. Dokumentation & Persistence
Nach erfolgreicher Validierung wurde der Systemzustand via Proxmox-Snapshot (`Phase_8_Final_Admin_Monitoring`) gesichert. Die Konfigurationsparameter sind fÃ¼r ein spÃ¤teres Rollout in die Produktionsumgebung dokumentiert.

---

# ğŸ“‚ Phase 9: Web-Infrastruktur & Netzwerk-Segmentierung

## ğŸ¯ Zielsetzung
Migration des Webservers in ein dediziertes Server-VLAN (DMZ-Konzept) zur Etablierung einer strikten Vertrauensgrenze. Fokus liegt auf der Implementierung des **Least-Privilege-Prinzips** durch granulare Firewall-RegelsÃ¤tze und die physische Trennung von Management- und Applikationstraffic.

---

## ğŸ—ï¸ 1. Netzwerk-Topologie & Migration
Zur Reduzierung der Broadcast-DomÃ¤nen und ErhÃ¶hung der Sicherheit wurde der Webserver in das isolierte **VLAN 20** migriert.

* **Interface-Konfiguration:** Umstellung auf eine statische Adressierung im Subnetz `10.0.20.0/24`.
* **Gateway-Struktur:** Die pfSense fungiert als zentraler Inter-VLAN-Router und Security-Gateway (`10.0.20.1`).
* **Adress-Validierung:** Erfolgreiche Bindung der IP `10.0.20.50` an das Ziel-Interface.

![Nachweis: Statische IP-Konfiguration (VLAN 20)](./img/webserver_neue_ip_adresse.png)

---

## ğŸš¦ 2. Firewall-HÃ¤rtung (Policy-Design)
Die Sicherheitsstrategie wurde von einer permissiven Struktur auf ein restriktives **Whitelist-Verfahren** umgestellt.

* **Management-Ebene (VLAN 10):** Autorisierter SSH-Zugriff (Port 22) ist exklusiv fÃ¼r dedizierte Management-Workstations freigegeben.
* **Client-Ebene (VLAN 30):** Implementierung einer "Service-Specific"-Rule. Windows-Clients dÃ¼rfen ausschlieÃŸlich Ã¼ber Port 80/443 mit dem Webserver kommunizieren.
* **Isolation:** Deaktivierung der *Default-Allow*-Rules. Jegliche Kommunikation zwischen den Segmenten (z. B. ICMP/Ping) wird unterbunden, um Reconnaissance-Versuche zu erschweren.

![Nachweis: Granulare Inbound-Rules im pfSense DMZ-Interface](./img/pfsense_DMZ_Rules.png)

---

## ğŸŒ 3. Dienst-Bereitstellung (LAMP-Stack)
Nach der erfolgreichen Netzwerk-Migration wurde die Applikationsebene auf dem Debian-Target initialisiert.

* **Service-IntegritÃ¤t:** Validierung des Apache2-Daemon-Status als "active (running)".
* **Content-Deployment:** Bereitstellung einer gehÃ¤rteten `index.html` zur Verifizierung des End-to-End-Zugriffs Ã¼ber VLAN-Grenzen hinweg.

---

## ğŸ§ª 4. Validierung (Compliance-Matrix)
Die FunktionalitÃ¤t der Segmentierung wurde durch eine strukturierte Testmatrix nachgewiesen:

| Test-Szenario | Vektor | Erwartetes Ergebnis | Ergebnis |
| :--- | :--- | :--- | :--- |
| **Applikations-Zugriff** | Win-11 â®• Web (Port 80) | HTTP 200 OK | **Pass** |
| **Administration** | Mint â®• Web (Port 22) | SSH-Handshake | **Pass** |
| **Inter-VLAN-Ping** | Win-11 â®• Webserver | Request Timeout | **Pass** |

![Nachweis: Restriktive Policy verhindert ICMP (Ping)](./img/Cmd_win_client_ping_webserver.png)

---

## ğŸ§¹ 5. Dokumentation & Status
Die VLAN-Zuweisungen und die Firewall-Objekte wurden in der pfSense-Konfiguration persistiert. Alle administrativen Zugangsdaten sind sicher in der `vault_passwords.yml` hinterlegt.

---

# ğŸ“‚ Phase 10: DNS-Infrastruktur & NamensauflÃ¶sung

## ğŸ¯ Zielsetzung
Implementierung einer internen NamensauflÃ¶sung (Split-DNS) zur Abstraktion der IP-Infrastruktur. Ziel ist die Bereitstellung eines konsistenten Zugriffs auf den Webserver Ã¼ber einen vollqualifizierten DomÃ¤nennamen (FQDN) innerhalb des isolierten Netzwerks.

---

## ğŸ› ï¸ 1. DNS-Zonendelegation (pfSense)
Die Steuerung der NamensauflÃ¶sung erfolgt zentral Ã¼ber den pfSense DNS-Resolver (Unbound).

* **Host Override:** Konfiguration eines statischen DNS-Eintrags fÃ¼r den FQDN `webserver.home.arpa`.
* **Mapping:** VerknÃ¼pfung des Hostnamens mit der statischen IP `10.0.20.50` (VLAN 20).
* **Vorteil:** ErmÃ¶glicht den Austausch der Hardware oder IP-Adressen ohne Anpassung der Client-Applikationen.

---

## ğŸš¦ 2. Protokoll-HÃ¤rtung (Firewall-Policies)
Um die DNS-IntegritÃ¤t zu gewÃ¤hrleisten, wurde der Zugriff auf den Resolver granular gesteuert.

* **UDP/53 Rule:** Implementierung einer Firewall-Regel im DMZ-Interface, die ausschlieÃŸlich DNS-Queries (`UDP Port 53`) an das Gateway (`10.0.30.1`) erlaubt.
* **Least Privilege:** Da die Standard-Policy auf *Deny All* steht, wurde der DNS-Zugriff als kritische AbhÃ¤ngigkeit explizit freigeschaltet.

---

## ğŸ’» 3. Client-Provisionierung
Die EndgerÃ¤te im Management-Netz wurden fÃ¼r die Nutzung der neuen DNS-AutoritÃ¤t konfiguriert.

* **Resolver-Konfiguration:** Statische Zuweisung der pfSense (`10.0.30.1`) als primÃ¤ren DNS-Server in den Netzwerkeinstellungen des Windows-Clients.
* **Domain-Suffix:** Sicherstellung der korrekten AuflÃ¶sung innerhalb der lokalen DomÃ¤ne `home.arpa`.

![Konfiguration: IPv4-Stack und DNS-Zuweisung](./img/win_client_netzwerkeinstellungen_ipadresse.png)

---

## ğŸ§ª 4. Validierung & KonnektivitÃ¤tstest
Der Nachweis der korrekten Implementierung erfolgte durch zweistufige Verifizierung:

1. **Resolver-Audit:** Erfolgreicher Query via `nslookup webserver.home.arpa` zur BestÃ¤tigung der korrekten IP-AuflÃ¶sung durch die pfSense.
2. **Applikations-Check:** VollstÃ¤ndiger HTTP-Handshake im Browser Ã¼ber den FQDN anstatt der IP-Adresse.

![Nachweis: Erfolgreiche NamensauflÃ¶sung im Browser](./img/browser_webserver_client_test.png)

---

## ğŸ§¹ 5. Dokumentation & Persistence
Der FQDN wurde als primÃ¤rer Zugangspunkt in der Asset-Liste vermerkt. Alle DNS-bezogenen Parameter sind konsistent mit der Netzwerk-Topologie dokumentiert.

---

# ğŸ“‚ Phase 11: Host-Hardening & Webserver-Absicherung

## ğŸ¯ Zielsetzung
Implementierung von Sicherheitsmechanismen auf Betriebssystem- und Applikationsebene (Host-Level-Hardening). Fokus liegt auf der Unterbindung von Information Disclosure, der Absicherung administrativer Schnittstellen und der Etablierung einer sekundÃ¤ren Firewall-Instanz.

---

## ğŸ”’ 1. Apache Service-Hardening
Zur Erschwerung von gezielten Exploits wurde die Informationspreisgabe des Webservers auf ein Minimum reduziert.

* **Information Obfuscation:** In der `security.conf` wurden `ServerTokens Prod` und `ServerSignature Off` gesetzt. Dies verhindert das Auslesen der genauen Apache-Version und OS-Details via HTTP-Header.
* **Directory Privacy:** Deaktivierung des Directory Listings (`-Indexes`), um das automatisierte Crawlen der Dateistruktur zu unterbinden.

![Nachweis: GehÃ¤rtete Apache-Sicherheitskonfiguration](./img/servertokens_security_conf.png)

---

## ğŸ”‘ 2. SSH-Infrastruktur-HÃ¤rtung
Der administrative Zugriff wurde nach dem "Least Privilege"-Prinzip und gÃ¤ngigen Best Practices abgesichert.

* **Account-Protection:** `PermitRootLogin no` erzwingt den Login Ã¼ber einen unprivilegierten Benutzer mit anschlieÃŸendem `sudo`, was Brute-Force-Angriffe auf den Root-Account eliminiert.
* **Brute-Force-PrÃ¤vention:** Begrenzung der Authentifizierungsversuche durch `MaxAuthTries 3`.

![Konfiguration: SSH-Daemon Sicherheitsrichtlinien](./img/sshd_config_datei.png)

---

## ğŸ›¡ï¸ 3. Defense in Depth (UFW-Implementierung)
ZusÃ¤tzlich zur zentralen pfSense-Firewall wurde eine lokale Instanz (Uncomplicated Firewall) als zweite Sicherheitsbarriere installiert.

* **Policy-Design:**
    * `Default Deny Incoming`: GrundsÃ¤tzliches Blockieren aller eingehenden Pakete.
    * `Allow 22/tcp`: Exklusiver Management-Zugang.
    * `Allow 80/tcp`: Autorisierter HTTP-Traffic.
* **Strategischer Vorteil:** Selbst bei einer Fehlkonfiguration der Netzwerk-Firewall bleibt der Host durch die lokale Policy geschÃ¼tzt.

![Status: Aktive Host-Firewall mit restriktivem Regelwerk](./img/ufw_status_verbose.png)

---

## ğŸ§ª 4. Validierung & Wirksamkeitsnachweis
Die HÃ¤rtungsmaÃŸnahmen wurden erfolgreich gegen die Sicherheitsvorgaben geprÃ¼ft:

1. **Header-Audit:** `curl -I` zeigt nur noch "Server: Apache" ohne Versionsnummern.
2. **Access-Test:** Root-Login via SSH wird systemseitig terminiert.
3. **Resilienz-Check:** Portscans bestÃ¤tigen, dass ausschlieÃŸlich die explizit freigegebenen Ports (80, 22) auf Anfragen reagieren.

---

## ğŸ§¹ 5. Dokumentation & Maintenance
Die Hardening-Konfigurationen wurden in das Master-Image Ã¼bernommen. Alle administrativen PasswÃ¶rter und SSH-Parameter sind konsistent in der `vault_passwords.yml` dokumentiert.

---

# ğŸ“‚ Phase 12: System-Hardening & Monitoring

## ğŸ¯ Zielsetzung
Absicherung der Systemlandschaft durch Minimierung der AngriffsflÃ¤che (Attack Surface Reduction) und Validierung der Netzwerksicherheit durch aktives Monitoring.

---

## ğŸš¦ 1. Network-Compliance (Egress Control)
Um die Wirksamkeit der Firewall-Policies zu garantieren, wurde der ausgehende Traffic des DMZ-Interfaces Ã¼ber die pfSense auditiert.

* **Befund:** Die Analyse der Echtzeit-Logs bestÃ¤tigt die korrekte Funktion der "Default Deny Rule".
* **Ergebnis:** Nicht autorisierte Verbindungsversuche (z. B. externe Telemetrie oder unbefugte DNS-Anfragen) werden konsistent verworfen. Dies verhindert effektiv die Datenexfiltration durch potenzielle Schadsoftware.

![Nachweis: Blockierter Traffic in pfSense Logs](./img/systemlogs_pfsense.png)

---

## ğŸ›¡ï¸ 2. Service-Hardening (Socket-Audit)
Der Webserver wurde einem technischen Audit unterzogen, um sicherzustellen, dass keine unnÃ¶tigen Dienste exponiert werden.

* **Audit-Methode:** Verifizierung der aktiven Netzwerk-Sockets mittels `ss -tulpn`.
* **Ergebnis MariaDB:** Die Datenbank ist strikt an `127.0.0.1` gebunden und somit fÃ¼r das Netzwerk unsichtbar.
* **Ergebnis SSH:** Der Zugriff ist systemseitig gehÃ¤rtet und wird zusÃ¤tzlich durch das Firewall-Regelwerk auf das Management-VLAN isoliert.

![Audit: Aktive Netzwerk-Sockets und Bind-Adressen](./img/ss_tulpn.png)

---

## ğŸ—ï¸ 3. Architektur-Validierung (Docker-Isolation)
Die Container-Infrastruktur wurde final auf ihre Sicherheitsvorgaben geprÃ¼ft.

* **Isolation:** Durch das Docker-interne Networking kommuniziert die Applikation Ã¼ber einen Proxy (Port 8080), wÃ¤hrend das Backend (MariaDB) vollstÃ¤ndig vom Host-Netz isoliert bleibt.
* **Persistenz:** Einsatz von Docker Volumes zur Trennung von flÃ¼chtigen Container-Daten und persistenten Applikationsdaten.

---

## âœ… 4. Fazit & Systemstatus
Die Implementierung erfÃ¼llt die modernen Sicherheitsstandards fÃ¼r gehÃ¤rtete Server-Umgebungen:

* **VLAN-Isolation:** Webserver erfolgreich in VLAN 20 segmentiert.
* **Least Privilege:** Firewall-Regelwerk auf das absolute Minimum reduziert.
* **IntegritÃ¤t:** Zentrales Logging und verschlÃ¼sselter Zugriff via FQDN (`webserver.home.arpa`) sind operativ.
---

# ğŸ“‚ Phase 13: Projektabschluss & Reflexion

## ğŸ¯ Zielsetzung
Abschlussbewertung der Systemarchitektur, Validierung der Sicherheitsvorgaben gegen das initiale Anforderungsprofil sowie kritische Analyse der ImplementierungshÃ¼rden.

---

## ğŸ“Š 1. Soll-Ist-Vergleich (Compliance-Check)
Der Abgleich der realisierten Infrastruktur mit dem Projektantrag bestÃ¤tigt die vollstÃ¤ndige Erreichung der Meilensteine:

* **Netzwerk-Segmentierung:** Erfolgreiche VLAN-Isolierung via pfSense. Die strikte Trennung zwischen DMZ und Webserver-VLAN ist aktiv.
* **Dienst-Isolation:** HÃ¤rtung des Datenbank-Backends durch Bind-Restriction auf `127.0.0.1`. Zugriff erfolgt ausschlieÃŸlich Ã¼ber autorisierte Applikations-Schnittstellen.
* **Containerisierung:** Erfolgreiche Migration des LAMP-Stacks in eine Docker-Umgebung mit dediziertem Port-Forwarding (8080/tcp -> 80/tcp).
* **TransportverschlÃ¼sselung:** Absicherung der Kommunikation Ã¼ber HTTPS (TLS 1.3) unter Verwendung des FQDN `webserver.home.arpa`.

---

## âš ï¸ 2. Kritische Reflexion & Incident-Analyse
Die Implementierungsphase lieferte durch technische Anomalien wichtige Erkenntnisse fÃ¼r den stabilen IT-Betrieb:

* **Inter-VLAN-Routing:** Initialer Kommunikationsabbruch trotz korrekter pfSense-Rules.
    * **Ursache:** Redundante Filterung durch die lokale Host-Firewall (`ufw`) auf dem Zielsystem.
    * **LÃ¶sung:** Konsolidierung der Firewall-Logik auf die pfSense (Zentralisierung) und Deaktivierung der lokalen `ufw`.
* **PKI-Herausforderungen:** Validierungsprobleme bei selbstsignierten Zertifikaten in modernen Browser-Umgebungen.
    * **LÃ¶sung:** Verifizierung der IntegritÃ¤t mittels CLI-Tools (`curl -k`) und temporÃ¤re Sicherheits-Overrides zur FunktionsprÃ¼fung.

---

## ğŸš€ 3. Fazit & Roadmap
Das Projekt demonstriert eine belastbare Kombination aus Netzwerksegmentierung und Applikations-Isolierung.

* **Skalierbarkeit:** Die Infrastruktur ist durch den Docker-Ansatz fÃ¼r zukÃ¼nftige Lastspitzen vorbereitet.
* **Optimierungspotenzial:** Integration einer automatisierten Zertifikats-CA (z. B. Letâ€™s Encrypt via ACME-Protokoll) sowie Implementierung eines zentralen Log-Managements (SIEM/ELK).
* **Abschlussurteil:** Die Architektur erfÃ¼llt die Anforderungen an eine moderne, gehÃ¤rtete Web-Infrastruktur und bildet eine solide Basis fÃ¼r den produktiven Betrieb.

---

## ğŸ§¹ 4. Dokumentation & Ãœbergabe
Alle relevanten Konfigurationsparameter, IP-Adressen und Credentials (hinterlegt in `vault_passwords.yml`) wurden in die finale Betriebsdokumentation Ã¼berfÃ¼hrt. Das Projekt wird hiermit in den Status **Abgeschlossen** versetzt.

---

# ğŸ“‚ Phase 14: Modernisierung mit Docker Compose

## ğŸ¯ Zielsetzung
Migration des manuellen Container-Handlings zu einer deklarativen Infrastruktur mittels Docker Compose. Fokus liegt auf der Automatisierung des Deployments, der Persistenz von Anwendungsdaten und der netzwerkseitigen Isolation der Datenbank.

---

## âš™ï¸ 1. Konfiguration & Orchestrierung
Die gesamte Service-Landschaft wurde in einer `docker-compose.yml` zentralisiert, um Konfigurations-Drift zu vermeiden.

* **Stack-Definition:** Orchestrierung eines PHP-Apache Webservers und einer MariaDB-Instanz.
* **Secret-Management:** Integration der sensiblen Variablen aus der `vault_passwords.yml` zur Laufzeit.
* **Netzwerk-Segmentierung:** Implementierung eines isolierten Bridge-Netzwerks. Die MariaDB ist explizit nicht nach auÃŸen (Host-Ports) gemappt, sondern nur via DNS-Alias fÃ¼r den Web-Container erreichbar.

---

## ğŸ’¾ 2. Datenkonsistenz & Volumes
Zur Entkoppelung von Applikationsdaten und Container-Lifecycle wurden persistente Mounts konfiguriert:

* **Bind Mount:** Mapping von `./html` (Host) auf `/var/www/html` (Container) fÃ¼r direkte Code-Manipulation und Persistenz der Web-Inhalte.
* **Named Volume:** Datenbank-Persistenz via `db_data` zur Sicherung der SQL-IntegritÃ¤t Ã¼ber Container-Restarts hinweg.

---

## ğŸ§ª 3. Validierung (Operational Status)
Die ÃœberprÃ¼fung der neuen Infrastruktur erfolgte durch standardisierte Tests:

* **Deployment-Test:** Verifizierung des Multi-Container-Starts mittels `docker compose up -d`.
* **KonnektivitÃ¤ts-Audit:** BestÃ¤tigung der internen Kommunikation zwischen Web-Service und DB-Backend Ã¼ber den Docker-internen Resolver.
* **Persistence-Audit:** Validierung der Datenerhaltung nach einem `docker compose down` und anschlieÃŸendem Rebuild.

---

## ğŸ§¹ 4. Dokumentation & Status
Der gesamte Stack ist nun versionskontrolliert und reproduzierbar. Die Verwaltung erfolgt ausschlieÃŸlich Ã¼ber die zentrale Compose-Datei, was die Skalierbarkeit und Wartbarkeit der Umgebung sicherstellt.

---

# ğŸ“‚ Phase 15: Automatisierung, Vault-Integration & Datenbank-Sicherung

## ğŸ¯ 1. Zielsetzung
Aufbau einer automatisierten Backup-Pipeline mit Ansible, um MariaDB-Datenbanken aus Docker-Containern zu sichern und verschlÃ¼sselt auf den Management-PC (Mint) zu Ã¼bertragen.

## ğŸ› ï¸ 2. Herausforderungen und LÃ¶sungen

### ğŸ”‘ A. Das Passwort-Paradoxon (Docker Volumes)
* **Problem:** MariaDB ignorierte Ã„nderungen am Passwort in der `docker-compose.yml`, da die Datenbank bereits mit einem alten Passwort im Volume `./db_data` initialisiert wurde.
* **Fehlermeldung:** `Error 1045: Access denied for user 'root'@'localhost'`.
* **LÃ¶sung:** Manueller Passwort-Reset Ã¼ber einen temporÃ¤ren Container im Safe-Mode (`--skip-grant-tables`). Durch das Laden der Privilegien (`FLUSH PRIVILEGES`) wurde das Passwort auf `123` synchronisiert.

### ğŸš« B. Port-Konflikt (Bind Failure)
* **Problem:** Der Web-Container konnte nach Wartungsarbeiten nicht starten, da Port `8080` bereits belegt war.
* **Ursache:** Verwaiste Container-Instanzen blockierten das Netzwerk-Interface.
* **LÃ¶sung:** Bereinigung der Docker-Umgebung mittels `docker compose down` und manuelles Stoppen hÃ¤ngender Prozesse.

### ğŸ” C. Vault-Struktur & Konfiguration
* **Problem:** Inkonsistente Variablenquellen zwischen YAML-Dateien.
* **LÃ¶sung:** Standardisierung auf `vault_passwords.yml` zur Speicherung sensibler Daten wie `db_password`.

## âŒ¨ï¸ 3. Die wichtigsten Befehle (Cheat Sheet)

### ğŸ“¦ Ansible & Vault
* **Playbook starten:** `ansible-playbook -i hosts.ini backup_mariadb.yml --ask-vault-pass --ask-become-pass`
* **Vault bearbeiten:** `ansible-vault edit vault_passwords.yml`
* **Backup-Inhalt prÃ¼fen:** `grep -i "CREATE TABLE" ~/backups/mariadb_backup_*.sql`

### ğŸ‹ Docker-Fehlerbehebung
* **Status prÃ¼fen:** `docker ps`
* **Stack aufrÃ¤umen:** `docker compose down`
* **Port gewaltsam befreien:** `sudo fuser -k 8080/tcp`

### ğŸ†˜ Der "Safe Mode" Reset (Notfall)
1. **Safe-Mode Start:** `docker run -d --name temp-fix -v $(pwd)/db_data:/var/lib/mysql mariadb --skip-grant-tables`
2. **Passwort Ã¤ndern:** `docker exec temp-fix mariadb -e "FLUSH PRIVILEGES; ALTER USER 'root'@'localhost' IDENTIFIED BY '123';"`
3. **Cleanup:** `docker stop temp-fix && docker rm temp-fix`

## ğŸš€ 4. Validierung der Ergebnisse
Der Erfolg wurde durch zwei PrÃ¼fungen bestÃ¤tigt:
1.  âœ… **Ansible Play Recap:** `changed=2` signalisierte den erfolgreichen Dump und Transfer.
2.  âœ… **InhaltsprÃ¼fung:** Ein manueller Scan bestÃ¤tigte die Existenz der Tabelle `logbuch` im SQL-Dump.

---
# ğŸ“‚ Phase 16: Security Auditing & Automated Hardening

In dieser Phase wurde der Fokus auf die messbare Sicherheit (Compliance) der Infrastruktur gelegt. Durch den Einsatz von professionellen Audit-Tools und Ansible-Automatisierung wurde der Sicherheitsstatus des Webservers analysiert und verbessert.

## ğŸ› ï¸ Verwendete Werkzeuge & Methoden
* **Lynis 3.x**: DurchfÃ¼hrung tiefgreifender Sicherheits-Scans auf dem Zielsystem.
* **Ansible**: Automatisierung der Installation, des Audits und der anschlieÃŸenden HÃ¤rtungsmaÃŸnahmen.

![Erfolgreiche Installation und Audit-Durchlauf von Lynis via Ansible](./img/Lynis_installed.png)
*Abbildung 1: BestÃ¤tigung der Lynis-Installation und des Audit-Tasks.*

## ğŸ“Š Performance-Metriken & Fortschritt
Der Erfolg der HÃ¤rtung wird durch den Hardening Index objektiviert. Durch gezielte MaÃŸnahmen konnten wir den Score steigern.

| Metrik | Baseline | Nach HÃ¤rtung | Status |
| :--- | :--- | :--- | :--- |
| **Hardening Index** | 68 | **70** | ğŸŸ¢ ErhÃ¶ht |
| **Docker Security** | 0 Warnings | 0 Warnings | ğŸŸ¢ Optimal |


## ğŸ›¡ï¸ Umgesetzte HÃ¤rtungs-MaÃŸnahmen (via Ansible)
Basierend auf den detaillierten Lynis-Suggestions () wurden folgende Konfigurationen automatisiert angepasst:

1. **SSH-IntegritÃ¤t**:
   * Anhebung des Log-Levels auf `VERBOSE` fÃ¼r detaillierte Forensik.
   * Deaktivierung von `AllowTcpForwarding`, um unautorisierte Tunnel zu unterbinden.
2. **System-Compliance**:
   * Implementierung eines rechtlichen Warnbanners (`/etc/issue`), um unbefugten Zugriff explizit zu untersagen.
3. **Betriebssicherheit**:
   * AuflÃ¶sung von Paketmanager-Konflikten und automatisierter Neustart kritischer Dienste nach Security-Patches.

## ğŸ Fazit
Die Infrastruktur erfÃ¼llt nun hÃ¶here Sicherheitsstandards. Der Anstieg des Hardening Index von 68 auf 70 beweist die Wirksamkeit der "Defense in Depth"-Strategie: Von der pfSense-Firewall Ã¼ber VLAN-Isolation bis hin zur gehÃ¤rteten Applikations-Ebene.

# ğŸ“‚ Phase 17: Advanced Hardening, Detection & Incident Response

In dieser Phase wurde die Sicherheit des Webservers von der Peripherie in den Kern des Betriebssystems verlagert. Der Fokus lag auf der Implementierung von Intrusion Detection Systemen (IDS) und der HÃ¤rtung von Kernel-Parametern.

## ğŸ› ï¸ Implementierte Sicherheits-Komponenten

### 1. Kernel-HÃ¤rtung (Network Stack)
Mittels Ansible wurden kritische `sysctl`-Parameter optimiert, um den Server gegen Netzwerk-Angriffe zu immunisieren.
* **IP-Spoofing Schutz**: Aktivierung von `rp_filter`.
* **ICMP-Sicherheit**: Ignorieren von Redirects und Broadcast-Pings zur Vermeidung von Man-in-the-Middle-Angriffen.
* **Hardware-Sicherheit**: Deaktivierung von USB-Storage-Treibern auf Software-Ebene.

### 2. Detection & Vulnerability Management
Um verdÃ¤chtige AktivitÃ¤ten frÃ¼hzeitig zu erkennen, wurden folgende Tools ausgerollt:
* **AIDE (File Integrity Monitoring)**: Erstellung einer Baseline der Systemdateien zur Erkennung von Manipulationen.
* **Rkhunter (Rootkit Scanner)**: DurchfÃ¼hrung von automatisierten Scans auf bekannte Backdoors und verdÃ¤chtige Dateitypen.
* **Process Accounting (acct)**: LÃ¼ckenlose Protokollierung von Benutzerbefehlen zur forensischen Analyse.

### 3. Incident Response mit Fail2Ban
Implementierung eines Schwellenwert-basierten Sperrsystems. IPs, die mehrfach an der SSH-Authentifizierung scheitern, werden automatisch auf Firewall-Ebene fÃ¼r 60 Minuten gesperrt.

## ğŸ“Š Ergebnisse & Audit-Verifizierung

Durch die Kombination aus Kernel-HÃ¤rtung und Monitoring-Tools konnte die Resilienz des Systems signifikant gesteigert werden.

| Metrik | Wert | Status |
| :--- | :--- | :--- |
| **Hardening Index (Neu)** | **72** | ğŸŸ¢ Steigerung gegenÃ¼ber Baseline |
| **Rootkit Check** | 0 Possible Rootkits | âœ… Bestanden |
| **Kernel Security** | Sysctl Optimized | âœ… Abgeschlossen |


![Rkhunter Audit Summary](./img/Rootkit-Scan.png)
*Zusammenfassung des Rootkit-Scans ohne kritische Funde.*

## ğŸ Fazit
Das System verfÃ¼gt nun Ã¼ber aktive Abwehrmechanismen. WÃ¤hrend Phase 16 die Compliance sicherstellte, hat Phase 17 die technische Tiefe fÃ¼r Detection & Response geschaffen.

# ğŸ“‚ Phase 18: IPS-Scharfschaltung & Management-HÃ¤rtung

## ğŸ¯ Zielsetzung
Versetzen von Suricata in den **Blocking-Mode** (IPS) und Absicherung des Admin-Zugangs, um ein versehentliches Aussperren ("Lockout") zu verhindern.

---

## ğŸ–¥ï¸ 1. DHCP & IP-Management
Um eine verlÃ¤ssliche Whitelist zu fÃ¼hren, wurde der Management-PC fest an eine IdentitÃ¤t gebunden.

* **Host-IdentitÃ¤t:** `mint-management`
* **MAC-Adresse:** `bc:24:11:02:83:6d`
* **Feste IP:** `10.0.10.52`
* **Netzwerk-Hygiene:** Der dynamische DHCP-Pool wurde auf `.60` bis `.100` verschoben, um Platz fÃ¼r statische Mappings zu schaffen und IP-Konflikte zu vermeiden.

---

## ğŸ§± 2. Suricata IPS-Konfiguration
Der Status wurde von "nur Beobachten" auf "aktives Blockieren" umgestellt.

* **Modus:** `Legacy Mode` (fÃ¼r maximale KompatibilitÃ¤t mit der Pass List).
* **Aktion:** `Block Offenders` aktiviert.
* **Sicherheitsnetz:** Zuweisung der `HomeLab_Whitelist` als **IP Pass List**, damit der Management-PC (`10.0.10.52`) niemals blockiert wird.
* **State-Kill:** `Kill States` aktiviert, um bÃ¶sartige Verbindungen sofort hart zu trennen.

---

## ğŸ§ª 3. Validierung (Der "Feuertest")
Die Wirksamkeit der Regeln wurde mit einem simulierten Angriff geprÃ¼ft.

* **Test-Vektor:** `curl -A "eicar" http://testmyids.com`
* **Erkennung:** Suricata meldete sofort `GPL ATTACK_RESPONSE id check returned root`.
* **Reaktion:** Die Angreifer-IP `217.160.0.187` wurde unmittelbar in die **Block-Liste** verschoben.
* **Erfolg:** Der Management-PC blieb dank der Pass List online und handlungsfÃ¤hig.
* 
  ![Erfolgreicher Block-Nachweis](./img/Suricata_block_list.png)
---

## ğŸ§¹ 4. Log- & System-Hygiene
* **Auto-Cleanup:** Log-Management aktiviert, um das Volllaufen der Festplatte zu verhindern.
* **Dashboard:** Integration der **Suricata Alerts** in das pfSense-HauptmenÃ¼ zur Echtzeit-Ãœberwachung.
* **Persistence:** Alle kritischen Zugangsdaten sind sicher in der `vault_passwords.yml` dokumentiert.

---

# ğŸ“‚ Phase 19: Web-Vulnerability Management & Server-HÃ¤rtung

## ğŸ¯ Zielsetzung
Identifizierung von Schwachstellen auf dem Webserver (`10.0.20.50`) mittels automatisierter Scans und DurchfÃ¼hrung gezielter HÃ¤rtungsmaÃŸnahmen zur Reduzierung der AngriffsflÃ¤che.

---

## ğŸ” 1. Vulnerability Scanning (Reconnaissance)
Ein technisches Audit mit `nmap --script vuln` deckte kritische Fehlkonfigurationen in der Web-Infrastruktur auf.

* **Zielsystem:** `10.0.20.50` (Debian Webserver)
* **Status:** Kritische SicherheitsmÃ¤ngel identifiziert.
* **Befund A:** Fehlende `HttpOnly` und `Secure` Flags bei Session-Cookies (PHPSESSID) auf den Ports 80, 443 und 8080.
* **Befund B:** Directory Enumeration ermÃ¶glichte das Auffinden der sensiblen Datei `/log.php`.

---

## ğŸ› ï¸ 2. Remediation (DurchgefÃ¼hrte HÃ¤rtung)
Um die gefundenen LÃ¼cken zu schlieÃŸen, wurden KonfigurationsÃ¤nderungen auf Applikations- und Serverebene vorgenommen.

* **Cookie-Sicherheit (PHP):** In der `/etc/php/8.4/apache2/php.ini` wurden `session.cookie_httponly = On` und `session.cookie_secure = On` aktiviert.
* **Zugriffsschutz (Apache):** Implementierung eines `<Files>`-Blocks in der `apache2.conf`, um den direkten Zugriff auf `/log.php` global zu verweigern.
* **SSH-Hygiene:** Validierung der SSH-Konfiguration; der Dienst war bereits korrekt mit `PermitRootLogin no` gegen direkten Root-Zugriff abgesichert.

---

## ğŸ§ª 3. Validierung (Der Wirksamkeitsnachweis)
Nach dem Neustart der Dienste wurden die MaÃŸnahmen vom Management-PC aus verifiziert.

* **Cookie-Check:** `curl -I` bestÃ¤tigt, dass Session-Cookies nun sicher mit `; secure; HttpOnly` Ã¼bertragen werden.
* **Zugriffs-Check:** Ein Aufruf der Datei `/log.php` resultiert nun unmittelbar in einem **403 Forbidden**.
* **Erfolg:** Die von Nmap gemeldeten Schwachstellen wurden effektiv eliminiert.

![Erfolgreicher HÃ¤rtungs-Nachweis](./img/forbidden.png)

---

## ğŸ§¹ 4. Dokumentation & Persistence
* **Standardisierung:** Alle Web-Sicherheitseinstellungen wurden in die globalen Konfigurationsdateien des Servers Ã¼bernommen.
* **Asset-Management:** Die Zugangsdaten fÃ¼r den Webserver und die Pfade zur VerschlÃ¼sselung sind sicher in der `vault_passwords.yml` hinter


# ğŸ“‚ Phase 20: Zentralisiertes Logging & SIEM-Vorbereitung

## ğŸ¯ Zielsetzung
Etablierung eines zentralen Log-Managements auf der pfSense, um Sicherheitsereignisse (wie die in Phase 19 provozierten 403-Fehler) vom Webserver in Echtzeit zu erfassen, zu korrelieren und fÃ¼r automatisierte Sperrmechanismen (IPS) nutzbar zu machen.

---

## ğŸ“¡ 1. Log-Infrastruktur (EmpfÃ¤nger & Sender)
Um eine saubere Trennung vom restlichen Firewall-Traffic zu gewÃ¤hrleisten, wurde ein dedizierter Log-Pfad Ã¼ber einen alternativen Port eingerichtet.

* **EmpfÃ¤nger (pfSense):** Konfiguration des `syslog-ng` Pakets auf Port `5140/UDP`.
* **Sender (Webserver):** Installation und Konfiguration von `syslog-ng` auf `10.0.20.50`, um lokale Apache-Fehler-Logs an die Firewall (`10.0.20.1`) weiterzuleiten.
* **Firewall-Regel:** Freischaltung von Port `5140` auf dem Interface **WEBSERVER**, um den eingehenden Log-Stream zu autorisieren.

---

## ğŸ› ï¸ 2. Konfiguration (Advanced Mapping)
Da die Standard-Objekte der pfSense geschÃ¼tzt sind, wurde eine manuelle Log-Kette innerhalb von `syslog-ng` implementiert, um Fehlerquellen (wie doppelte Datei-Zugriffe) zu vermeiden.

* **Custom Source:** Erstellung des Objekts `s_webserver_remote`, das explizit auf Netzwerk-Pakete an Port 5140 lauscht.
* **Log Logic:** VerknÃ¼pfung der Remote-Quelle mit dem Ziel-Pfad `/var/syslog-ng/default.log` Ã¼ber das Log-Objekt `l_webserver_connect`.
* **Dienst-Validierung:** Sicherstellung des Dienststatus Ã¼ber `Status > Services`, nachdem Syntax-Fehler in der `syslog-ng.conf` behoben wurden.

---

## ğŸ§ª 3. Validierung (Live-Monitoring)
Der Erfolg der zentralen Protokollierung wurde durch den Abgleich von Webserver-Events und Firewall-Anzeige nachgewiesen.

* **Event-Trigger:** Manuelle AuslÃ¶sung der in Phase 19 gehÃ¤rteten ZugriffsbeschrÃ¤nkung auf `log.php`.
* **Log Viewer:** Erfolgreiche Sichtung der Apache-Fehlermeldungen (`authz_core:error`) direkt in der pfSense-WeboberflÃ¤che.
* **Filter-Check:** Verifikation der Sichtbarkeit mittels Hostnamen-Filter (`webserver`), wodurch relevante Angriffsversuche sofort isoliert werden kÃ¶nnen.

![Zentraler Log-Eingang der Webserver-Fehler](./img/Logviewer.png)

---

## ğŸ§¹ 4. Dokumentation & Persistence
* **Fehlerbehebung:** Dokumentation der Syntax-Korrekturen (Vermeidung von `conflicting persist-names`) zur zukÃ¼nftigen Wartung des Log-Dienstes.
* **Vault-Integration:** Hinterlegung der fÃ¼r die API-Anbindung notwendigen Credentials in der Datei `vault_passwords.yml`.
* **Ausblick:** Die nun flieÃŸenden Logdaten dienen als Trigger fÃ¼r Phase 21 (Automatisiertes Blocking mittels pfBlockerNG).
